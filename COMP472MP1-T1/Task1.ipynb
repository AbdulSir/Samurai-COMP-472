{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63401305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8d5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files(\"BBC/\", encoding=\"latin-1\", decode_error=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec8e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0b2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}\n"
     ]
    }
   ],
   "source": [
    "labels_str = np.array(data.target_names)[labels]\n",
    "class_dict=dict(zip(labels_str, counts))\n",
    "\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e012141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(labels_str, counts)\n",
    "plt.savefig(\" bbc-distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c9e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size= 0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb86e96",
   "metadata": {},
   "source": [
    "# MultinomialNB default values, try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6e55d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', stop_words=\"english\", decode_error=\"ignore\")\n",
    "corpus = vectorizer.fit_transform(x_train)\n",
    "cls = MultinomialNB()\n",
    "cls.fit(vectorizer.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141d19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_props = \"\"\n",
    "class_instances = [0,0,0,0,0]\n",
    "\n",
    "for i in y_train:\n",
    "    if (i==0):\n",
    "        class_instances[0]+=1;\n",
    "    elif (i==1):\n",
    "        class_instances[1]+=1\n",
    "    elif (i==2):\n",
    "        class_instances[2]+=1\n",
    "    elif (i==3):\n",
    "        class_instances[3]+=1\n",
    "    elif (i==4):\n",
    "        class_instances[4]+=1\n",
    "        \n",
    "for i in range(len(class_instances)):\n",
    "    class_instances[i] = '{0:.5g}'.format(100*class_instances[i]/len(y_train))\n",
    "    class_props += \"Prior probability of \" + labels_str[i] + \": \" + class_instances[i] + \"%\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0592d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counts = pd.DataFrame(corpus.toarray(), index=y_train, columns=vectorizer.get_feature_names())\n",
    "class_classification = []\n",
    "class_word_ctr = [0,0,0,0,0]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    class_classification.append(counts.loc[counts.index==i].sum(axis=0))\n",
    "    for j in class_classification[i]:\n",
    "        class_word_ctr[i] += class_classification[i][j]\n",
    "        \n",
    "class_word_tokens = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    class_word_tokens += \"The number of word-tokens in \" + labels_str[i] + \": \" + str(class_word_ctr[i]) + \"\\n\" \n",
    "class_word_tokens += \"\\nThe number of word-tokens in the entire corpus is: \" + str(np.sum(class_word_ctr)) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d220cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_freq_words_ctr = [0,0,0,0,0]\n",
    "one_freq_words_ctr = 0\n",
    "\n",
    "for i in range(5):\n",
    "    for j in class_classification[i]:\n",
    "        if class_classification[i][j]==0:\n",
    "            zero_freq_words_ctr[i] += 1\n",
    "        if class_classification[i][j]==1:\n",
    "            one_freq_words_ctr += 1\n",
    "            \n",
    "zero_freq_words_per = ['{0:.3f}'.format(100*zero_freq_words_ctr[i]/class_word_ctr[i]) for i in range(5)]\n",
    "one_freq_words_per = '{0:.3f}'.format(100*one_freq_words_ctr/np.sum(class_word_ctr))\n",
    "\n",
    "words_number_and_per_str = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    words_number_and_per_str += \"The number and percentage of words with a frequency of zero in \" + labels_str[i] + \": Number-> \" + str(zero_freq_words_ctr[i]) + \"\\tPercentage-> \" + str(zero_freq_words_per[i]) + \"%\\n\"\n",
    "    \n",
    "words_number_and_per_str += \"\\nThe number and percentage of words with a frequency of one in the entire corpus is: Number-> \" + str(one_freq_words_ctr) + \"\\tPercentage-> \" + str(one_freq_words_per) + \"%\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21c9d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "word_list = vectorizer.get_feature_names();    \n",
    "count_list = corpus.toarray().sum(axis=0)\n",
    "corpus_dict = dict(zip(word_list,count_list))\n",
    "\n",
    "prog_prop = '{0:.3f}'.format(math.log(corpus_dict['programming']/np.sum(class_word_ctr)))\n",
    "\n",
    "food_prop = '{0:.3f}'.format(math.log(corpus_dict['food']/np.sum(class_word_ctr)))\n",
    "\n",
    "fav_words_str = \"The log-prop of Programming is: \" + prog_prop + \"\\nThe log-prop of Food is: \" + food_prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bbff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "y_pred = cls.predict(vectorizer.transform(x_test))\n",
    "with open(\"bbc-performance.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"********************************************************\\nMultinomialNB default values, try 1\\n\\n\")\n",
    "    myfile.write(\"\\nb)\\nConfusion Matrix:\\n\\n\" + str(confusion_matrix(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\nc & d)\\nAccuracy:\\t\" + str(accuracy_score(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\n\" + str(classification_report(y_test, y_pred, target_names=data.target_names)))\n",
    "    myfile.write(\"\\n\\ne)\\n\" + class_props)\n",
    "    myfile.write(\"\\n\\nf)\\nThe size of the vocabulary: \" + str(len(corpus_dict)))\n",
    "    myfile.write(\"\\n\\ng & h)\\n\" + class_word_tokens)\n",
    "    myfile.write(\"\\n\\ni & j)\\n\" + words_number_and_per_str)\n",
    "    myfile.write(\"\\n\\nk)\\n\" + fav_words_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17469594",
   "metadata": {},
   "source": [
    "# MultinomialNB default values, try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f9db3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', stop_words=\"english\", decode_error=\"ignore\")\n",
    "corpus = vectorizer.fit_transform(x_train)\n",
    "cls = MultinomialNB()\n",
    "cls.fit(vectorizer.transform(x_train), y_train)\n",
    "\n",
    "class_props = \"\"\n",
    "class_instances = [0,0,0,0,0]\n",
    "\n",
    "for i in y_train:\n",
    "    if (i==0):\n",
    "        class_instances[0]+=1;\n",
    "    elif (i==1):\n",
    "        class_instances[1]+=1\n",
    "    elif (i==2):\n",
    "        class_instances[2]+=1\n",
    "    elif (i==3):\n",
    "        class_instances[3]+=1\n",
    "    elif (i==4):\n",
    "        class_instances[4]+=1\n",
    "        \n",
    "for i in range(len(class_instances)):\n",
    "    class_instances[i] = '{0:.5g}'.format(100*class_instances[i]/len(y_train))\n",
    "    class_props += \"Prior probability of \" + labels_str[i] + \": \" + class_instances[i] + \"%\\n\"\n",
    "\n",
    "counts = pd.DataFrame(corpus.toarray(), index=y_train, columns=vectorizer.get_feature_names())\n",
    "\n",
    "class_classification = []\n",
    "class_word_ctr = [0,0,0,0,0]\n",
    "for i in range(5):\n",
    "    class_classification.append(counts.loc[counts.index==i].sum(axis=0))\n",
    "    for j in class_classification[i]:\n",
    "        class_word_ctr[i] += class_classification[i][j]\n",
    "\n",
    "class_word_tokens = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    class_word_tokens += \"The number of word-tokens in \" + labels_str[i] + \": \" + str(class_word_ctr[i]) + \"\\n\" \n",
    "class_word_tokens += \"\\nThe number of word-tokens in the entire corpus is: \" + str(np.sum(class_word_ctr)) + \"\\n\"\n",
    "\n",
    "zero_freq_words_ctr = [0,0,0,0,0]\n",
    "one_freq_words_ctr = 0\n",
    "\n",
    "for i in range(5):\n",
    "    for j in class_classification[i]:\n",
    "        if class_classification[i][j]==0:\n",
    "            zero_freq_words_ctr[i] += 1\n",
    "        if class_classification[i][j]==1:\n",
    "            one_freq_words_ctr += 1\n",
    "            \n",
    "zero_freq_words_per = ['{0:.3f}'.format(100*zero_freq_words_ctr[i]/class_word_ctr[i]) for i in range(5)]\n",
    "one_freq_words_per = '{0:.3f}'.format(100*one_freq_words_ctr/np.sum(class_word_ctr))\n",
    "\n",
    "words_number_and_per_str = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    words_number_and_per_str += \"The number and percentage of words with a frequency of zero in \" + labels_str[i] + \": Number-> \" + str(zero_freq_words_ctr[i]) + \"\\tPercentage-> \" + str(zero_freq_words_per[i]) + \"%\\n\"\n",
    "    \n",
    "words_number_and_per_str += \"\\nThe number and percentage of words with a frequency of zero one in the entire corpus is: Number-> \" + str(one_freq_words_ctr) + \"\\tPercentage-> \" + str(one_freq_words_per) + \"%\\n\"\n",
    "\n",
    "word_list = vectorizer.get_feature_names();    \n",
    "count_list = corpus.toarray().sum(axis=0)\n",
    "corpus_dict = dict(zip(word_list,count_list))\n",
    "\n",
    "prog_prop = '{0:.3f}'.format(math.log(corpus_dict['programming']/np.sum(class_word_ctr)))\n",
    "\n",
    "food_prop = '{0:.3f}'.format(math.log(corpus_dict['food']/np.sum(class_word_ctr)))\n",
    "\n",
    "fav_words_str = \"The log-prop of Programming is: \" + prog_prop + \"\\nThe log-prop of Food is: \" + food_prop\n",
    "\n",
    "y_pred = cls.predict(vectorizer.transform(x_test))\n",
    "with open(\"bbc-performance.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"\\n\\n********************************************************\\nMultinomialNB default values, try 2\\n\\n\")\n",
    "    myfile.write(\"\\nb)\\nConfusion Matrix:\\n\\n\" + str(confusion_matrix(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\nc & d)\\nAccuracy:\\t\" + str(accuracy_score(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\n\" + str(classification_report(y_test, y_pred, target_names=data.target_names)))\n",
    "    myfile.write(\"\\n\\ne)\\n\" + class_props)\n",
    "    myfile.write(\"\\n\\nf)\\nThe size of the vocabulary: \" + str(len(corpus_dict)))\n",
    "    myfile.write(\"\\n\\ng & h)\\n\" + class_word_tokens)\n",
    "    myfile.write(\"\\n\\ni & j)\\n\" + words_number_and_per_str)\n",
    "    myfile.write(\"\\n\\nk)\\n\" + fav_words_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707ecb1",
   "metadata": {},
   "source": [
    "# MultinomialNB default values, try 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f8b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', stop_words=\"english\", decode_error=\"ignore\")\n",
    "corpus = vectorizer.fit_transform(x_train)\n",
    "cls = MultinomialNB(alpha=0.0001)\n",
    "cls.fit(vectorizer.transform(x_train), y_train)\n",
    "\n",
    "class_props = \"\"\n",
    "class_instances = [0,0,0,0,0]\n",
    "\n",
    "for i in y_train:\n",
    "    if (i==0):\n",
    "        class_instances[0]+=1;\n",
    "    elif (i==1):\n",
    "        class_instances[1]+=1\n",
    "    elif (i==2):\n",
    "        class_instances[2]+=1\n",
    "    elif (i==3):\n",
    "        class_instances[3]+=1\n",
    "    elif (i==4):\n",
    "        class_instances[4]+=1\n",
    "        \n",
    "for i in range(len(class_instances)):\n",
    "    class_instances[i] = '{0:.5g}'.format(100*class_instances[i]/len(y_train))\n",
    "    class_props += \"Prior probability of \" + labels_str[i] + \": \" + class_instances[i] + \"%\\n\"\n",
    "\n",
    "counts = pd.DataFrame(corpus.toarray(), index=y_train, columns=vectorizer.get_feature_names())\n",
    "\n",
    "class_classification = []\n",
    "class_word_ctr = [0,0,0,0,0]\n",
    "for i in range(5):\n",
    "    class_classification.append(counts.loc[counts.index==i].sum(axis=0))\n",
    "    for j in class_classification[i]:\n",
    "        class_word_ctr[i] += class_classification[i][j]\n",
    "        \n",
    "class_word_tokens = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    class_word_tokens += \"The number of word-tokens in \" + labels_str[i] + \": \" + str(class_word_ctr[i]) + \"\\n\" \n",
    "class_word_tokens += \"\\nThe number of word-tokens in the entire corpus is: \" + str(np.sum(class_word_ctr)) + \"\\n\"\n",
    "\n",
    "zero_freq_words_ctr = [0,0,0,0,0]\n",
    "one_freq_words_ctr = 0\n",
    "\n",
    "for i in range(5):\n",
    "    for j in class_classification[i]:\n",
    "        if class_classification[i][j]==0:\n",
    "            zero_freq_words_ctr[i] += 1\n",
    "        if class_classification[i][j]==1:\n",
    "            one_freq_words_ctr += 1\n",
    "            \n",
    "zero_freq_words_per = ['{0:.3f}'.format(100*zero_freq_words_ctr[i]/class_word_ctr[i]) for i in range(5)]\n",
    "one_freq_words_per = '{0:.3f}'.format(100*one_freq_words_ctr/np.sum(class_word_ctr))\n",
    "\n",
    "words_number_and_per_str = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    words_number_and_per_str += \"The number and percentage of words with a frequency of zero in \" + labels_str[i] + \": Number-> \" + str(zero_freq_words_ctr[i]) + \"\\tPercentage-> \" + str(zero_freq_words_per[i]) + \"%\\n\"\n",
    "    \n",
    "words_number_and_per_str += \"\\nThe number and percentage of words with a frequency of zero one in the entire corpus is: Number-> \" + str(one_freq_words_ctr) + \"\\tPercentage-> \" + str(one_freq_words_per) + \"%\\n\"\n",
    "\n",
    "word_list = vectorizer.get_feature_names();    \n",
    "count_list = corpus.toarray().sum(axis=0)\n",
    "corpus_dict = dict(zip(word_list,count_list))\n",
    "\n",
    "prog_prop = '{0:.3f}'.format(math.log(corpus_dict['programming']/np.sum(class_word_ctr)))\n",
    "\n",
    "food_prop = '{0:.3f}'.format(math.log(corpus_dict['food']/np.sum(class_word_ctr)))\n",
    "\n",
    "fav_words_str = \"The log-prop of Programming is: \" + prog_prop + \"\\nThe log-prop of Food is: \" + food_prop\n",
    "\n",
    "y_pred = cls.predict(vectorizer.transform(x_test))\n",
    "with open(\"bbc-performance.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"\\n\\n********************************************************\\nMultinomialNB default values, try 3\\n\\n\")\n",
    "    myfile.write(\"\\nb)\\nConfusion Matrix:\\n\\n\" + str(confusion_matrix(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\nc & d)\\nAccuracy:\\t\" + str(accuracy_score(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\n\" + str(classification_report(y_test, y_pred, target_names=data.target_names)))\n",
    "    myfile.write(\"\\n\\ne)\\n\" + class_props)\n",
    "    myfile.write(\"\\n\\nf)\\nThe size of the vocabulary: \" + str(len(corpus_dict)))\n",
    "    myfile.write(\"\\n\\ng & h)\\n\" + class_word_tokens)\n",
    "    myfile.write(\"\\n\\ni & j)\\n\" + words_number_and_per_str)\n",
    "    myfile.write(\"\\n\\nk)\\n\" + fav_words_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1b859",
   "metadata": {},
   "source": [
    "# MultinomialNB default values, try 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "389b7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', stop_words=\"english\", decode_error=\"ignore\")\n",
    "corpus = vectorizer.fit_transform(x_train)\n",
    "cls = MultinomialNB(alpha=0.9)\n",
    "cls.fit(vectorizer.transform(x_train), y_train)\n",
    "\n",
    "class_props = \"\"\n",
    "class_instances = [0,0,0,0,0]\n",
    "\n",
    "for i in y_train:\n",
    "    if (i==0):\n",
    "        class_instances[0]+=1;\n",
    "    elif (i==1):\n",
    "        class_instances[1]+=1\n",
    "    elif (i==2):\n",
    "        class_instances[2]+=1\n",
    "    elif (i==3):\n",
    "        class_instances[3]+=1\n",
    "    elif (i==4):\n",
    "        class_instances[4]+=1\n",
    "        \n",
    "for i in range(len(class_instances)):\n",
    "    class_instances[i] = '{0:.5g}'.format(100*class_instances[i]/len(y_train))\n",
    "    class_props += \"Prior probability of \" + labels_str[i] + \": \" + class_instances[i] + \"%\\n\"\n",
    "\n",
    "counts = pd.DataFrame(corpus.toarray(), index=y_train, columns=vectorizer.get_feature_names())\n",
    "\n",
    "class_classification = []\n",
    "class_word_ctr = [0,0,0,0,0]\n",
    "for i in range(5):\n",
    "    class_classification.append(counts.loc[counts.index==i].sum(axis=0))\n",
    "    for j in class_classification[i]:\n",
    "        class_word_ctr[i] += class_classification[i][j]\n",
    "\n",
    "class_word_tokens = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    class_word_tokens += \"The number of word-tokens in \" + labels_str[i] + \": \" + str(class_word_ctr[i]) + \"\\n\" \n",
    "class_word_tokens += \"\\nThe number of word-tokens in the entire corpus is: \" + str(np.sum(class_word_ctr)) + \"\\n\"\n",
    "\n",
    "zero_freq_words_ctr = [0,0,0,0,0]\n",
    "one_freq_words_ctr = 0\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    for j in class_classification[i]:\n",
    "        if class_classification[i][j]==0:\n",
    "            zero_freq_words_ctr[i] += 1\n",
    "        if class_classification[i][j]==1:\n",
    "            one_freq_words_ctr += 1\n",
    "            \n",
    "zero_freq_words_per = ['{0:.3f}'.format(100*zero_freq_words_ctr[i]/class_word_ctr[i]) for i in range(5)]\n",
    "one_freq_words_per = '{0:.3f}'.format(100*one_freq_words_ctr/np.sum(class_word_ctr))\n",
    "\n",
    "words_number_and_per_str = \"\"\n",
    "\n",
    "for i in range(5):\n",
    "    words_number_and_per_str += \"The number and percentage of words with a frequency of zero in \" + labels_str[i] + \": Number-> \" + str(zero_freq_words_ctr[i]) + \"\\tPercentage-> \" + str(zero_freq_words_per[i]) + \"%\\n\"\n",
    "    \n",
    "words_number_and_per_str += \"\\nThe number and percentage of words with a frequency of zero one in the entire corpus is: Number-> \" + str(one_freq_words_ctr) + \"\\tPercentage-> \" + str(one_freq_words_per) + \"%\\n\"\n",
    "\n",
    "word_list = vectorizer.get_feature_names();    \n",
    "count_list = corpus.toarray().sum(axis=0)\n",
    "corpus_dict = dict(zip(word_list,count_list))\n",
    "\n",
    "prog_prop = '{0:.3f}'.format(math.log(corpus_dict['programming']/np.sum(class_word_ctr)))\n",
    "\n",
    "food_prop = '{0:.3f}'.format(math.log(corpus_dict['food']/np.sum(class_word_ctr)))\n",
    "\n",
    "fav_words_str = \"The log-prop of Programming is: \" + prog_prop + \"\\nThe log-prop of Food is: \" + food_prop\n",
    "\n",
    "y_pred = cls.predict(vectorizer.transform(x_test))\n",
    "with open(\"bbc-performance.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"\\n\\n********************************************************\\nMultinomialNB default values, try 4\\n\\n\")\n",
    "    myfile.write(\"\\nb)\\nConfusion Matrix:\\n\\n\" + str(confusion_matrix(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\nc & d)\\nAccuracy:\\t\" + str(accuracy_score(y_test, y_pred)))\n",
    "    myfile.write(\"\\n\\n\" + str(classification_report(y_test, y_pred, target_names=data.target_names)))\n",
    "    myfile.write(\"\\n\\ne)\\n\" + class_props)\n",
    "    myfile.write(\"\\n\\nf)\\nThe size of the vocabulary: \" + str(len(corpus_dict)))\n",
    "    myfile.write(\"\\n\\ng & h)\\n\" + class_word_tokens)\n",
    "    myfile.write(\"\\n\\ni & j)\\n\" + words_number_and_per_str)\n",
    "    myfile.write(\"\\n\\nk)\\n\" + fav_words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772a4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
